{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Miniproject1_combo.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HungYangChang/ECSE551/blob/master/Miniproject1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLdbMJpptgiB"
      },
      "source": [
        "# Import relevant modules\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z22guoEOgiKx"
      },
      "source": [
        "# Read Data Sets (Bankrupcy and Hepatitis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FWa6B074nO4"
      },
      "source": [
        "# Load bankruptcy data\n",
        "url = \"https://raw.githubusercontent.com/jonarsenault/ecse551data/master/bankrupcy.csv\"\n",
        "bank_data = pd.read_csv(url)\n",
        "\n",
        "# Display some of the data\n",
        "print(bank_data.head())\n",
        "\n",
        "# Print size of data\n",
        "bank_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGv3KRgWuMGM"
      },
      "source": [
        "# Load hepatitis data\n",
        "url = \"https://raw.githubusercontent.com/jonarsenault/ecse551data/master/hepatitis.csv\"\n",
        "hep_data = pd.read_csv(url)\n",
        "\n",
        "# Display some of the data\n",
        "print(hep_data.head())\n",
        "\n",
        "# Print size of data\n",
        "hep_data.shape\n",
        "\n",
        "# Indices of numerical and categorical features\n",
        "index_cat_columns_raw = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18])\n",
        "index_num_columns_raw = np.array([0, 13, 14, 15, 16, 17])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxY6ov7qKnLF"
      },
      "source": [
        "# Define utility functions for logistic regression\n",
        "- shuffle_data\n",
        "- splitdata\n",
        "- standardization\n",
        "- sigmoid\n",
        "- log_transform\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH2Ra3N6Klmd"
      },
      "source": [
        "def shuffle_data(X, y, random_seed=None):\n",
        "  \"\"\"Shuffle the data to randomize tests\"\"\"\n",
        "\n",
        "  if random_seed is not None:\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "  # Copy data\n",
        "  X_original_copy = X.copy()\n",
        "  y_original_copy = y.copy()\n",
        "\n",
        "  # Concatenate and shuffle\n",
        "  full_array = np.concatenate((X_original_copy, y_original_copy), axis=1)\n",
        "  np.random.shuffle(full_array)\n",
        "\n",
        "  # Split into features and labels\n",
        "  X_shuffle = full_array[:,:-1]\n",
        "  y_shuffle  = full_array[:,[-1]]\n",
        "\n",
        "  return X_shuffle, y_shuffle\n",
        "\n",
        "def splitdata(X, y, perc_training, random_flag=True):\n",
        "  \"\"\"Split data into training and testing set (by rows(observations)) by taking a constant set\"\"\"\n",
        "\n",
        "  num_rows = X.shape[0]\n",
        "  num_rows_train = int(num_rows * perc_training )\n",
        "  num_rows_test = num_rows - num_rows_train\n",
        "\n",
        "  X_train = X[:num_rows_train, :]\n",
        "  X_test =  X[num_rows_train:, :]\n",
        "  y_train = y[:num_rows_train]\n",
        "  y_test = y[num_rows_train:]\n",
        "\n",
        "  return X_train, y_train, X_test, y_test\n",
        "\n",
        "def standardization(data, training_data):\n",
        "  \"\"\"Standardize each column of input data\"\"\"\n",
        "\n",
        "  data_standardized = (data - training_data.mean(axis=0))/training_data.std(axis=0)\n",
        "\n",
        "  return data_standardized\n",
        "\n",
        "def sigmoid(x):\n",
        "  \"\"\"Apply logistic sigmoid function to input\"\"\"\n",
        "\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def log_transform(X_data, feature, replace=True, bank=False):\n",
        "  \"\"\"Perform log transform if feature may not be normally distributed\"\"\"\n",
        "\n",
        "  if replace:\n",
        "    # Features to be replaced with their log transform\n",
        "\n",
        "    for i in feature:\n",
        "      # Check if mean is close to median\n",
        "      variance = np.absolute(np.mean(X_data[:,i])-np.median(X_data[:,i]))/np.mean(X_data[:,i])\n",
        "      if (variance > 0.1):\n",
        "        if bank:\n",
        "          # For bankruptcy data, ensure data is positive\n",
        "          X_data[:,i] = X_data[:,i] + np.absolute(np.min(X_data[:,i]))+0.5\n",
        "          X_data[:,i] = np.log(X_data[:,i])\n",
        "        else:\n",
        "          X_data[:,i] = np.log(X_data[:,i])\n",
        "    \n",
        "  else:\n",
        "    # Append log transform to exisitng features\n",
        "\n",
        "    for i in feature:\n",
        "      # Check if mean is close to median\n",
        "      variance = np.absolute(np.mean(X_data[:,i])-np.median(X_data[:,i]))/np.mean(X_data[:,i])\n",
        "      if (variance > 0.1):\n",
        "        if bank:\n",
        "          # For bankruptcy data, ensure data is positive\n",
        "          X_data[:,i] = X_data[:,i] + np.absolute(np.min(X_data[:,i]))+0.5\n",
        "          X_data = np.insert(X_data, X_data.shape[1]-1, np.log(X_data[:,i]), axis=1)\n",
        "        else:\n",
        "          X_data = np.insert(X_data, X_data.shape[1]-1, np.log(X_data[:,i]), axis=1)\n",
        "\n",
        "  return X_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtGtoeByFeZn"
      },
      "source": [
        "# Logistic Classifier Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVGQ1MC1FcU5"
      },
      "source": [
        "class LogisticClassifier():\n",
        "  \"\"\"Class defining a logistic classifier\"\"\"\n",
        "  \n",
        "  def __init__(self, weights):\n",
        "    \"\"\"Constructor\"\"\"\n",
        "\n",
        "    self._w = weights\n",
        "  \n",
        "  def fit(self, x_train, y_train, max_iters, tolerance=1e-2, print_results=True, store_w_iteration= False, learning_rate=\"dependent\"):\n",
        "    \"\"\"Fit a logistic regression model to data\"\"\"\n",
        "\n",
        "    # Lists to store (1) weights at each iteration or (2) just last weights\n",
        "    weight_store_fit = []\n",
        "\n",
        "    iteration = 1\n",
        "    delta_weights = 1e6\n",
        "\n",
        "    while (delta_weights > tolerance) & (iteration < max_iters):\n",
        "\n",
        "      # Set learning rate for this iteration\n",
        "      if learning_rate == \"dependent\":\n",
        "        alpha = 1/(1 + iteration)\n",
        "      elif learning_rate == \"small\":\n",
        "        alpha = 0.1\n",
        "      else:\n",
        "        alpha = 0.5\n",
        "\n",
        "      # Store current weights before updating\n",
        "      weights_previous = self._w\n",
        "\n",
        "      # Compute gradient of cross-entropy loss\n",
        "      gradient = np.sum(\n",
        "      x_train * (y_train - sigmoid(np.dot(x_train, weights_previous))), axis=0\n",
        "      ).reshape(-1, 1)\n",
        "\n",
        "      # Update weights\n",
        "      self._w = weights_previous + alpha * gradient\n",
        "\n",
        "      # Compute change in weights\n",
        "      delta_weights = np.linalg.norm(self._w - weights_previous) ** 2    \n",
        "\n",
        "      if (store_w_iteration == True):\n",
        "      # Store weights at each iteration\n",
        "        weight_store_fit.append(self._w.flatten()) #TODOO\n",
        "\n",
        "      iteration += 1  \n",
        "\n",
        "    if (store_w_iteration == False):\n",
        "    # Store final weights \n",
        "      weight_store_fit.append(self._w.flatten()) \n",
        "      # print (\"Final weights:\", weight_store_fit)\n",
        "\n",
        "    # Compute training accuracy\n",
        "    y_pred_train = self.predict(x_train)\n",
        "\n",
        "    accuracy = self.accu_eval(y_train, y_pred_train)\n",
        "\n",
        "    if print_results:\n",
        "      if iteration==max_iters:\n",
        "        print (f\"Failed to converge in {max_iters} iterations\")\n",
        "      else:\n",
        "        print (f\"Model converged in {max_iters} iterations\")\n",
        "      print(f\"Training accuracy: {100*accuracy:.2f}\")\n",
        "\n",
        "    return accuracy, iteration, weight_store_fit\n",
        "\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"Predict the class labels of a given set of samples\"\"\"\n",
        "\n",
        "    # Decision boundary\n",
        "    decision_boundary = 0.5    \n",
        "\n",
        "    # Obtain probability of each sample\n",
        "    y_pred_prob = sigmoid(np.dot(X, self._w))\n",
        "\n",
        "    # Assign class labels based on decision boundary\n",
        "    y_pred = np.where(y_pred_prob < decision_boundary, 0, 1)\n",
        "\n",
        "    return y_pred\n",
        "  \n",
        "\n",
        "  def accu_eval(self, y_true, y_pred):\n",
        "    \"\"\"Compute accuracy of model\"\"\" \n",
        "\n",
        "    accuracy = np.count_nonzero(y_true == y_pred) / len(y_true)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "  def cross_entropy_loss(self, x_test, y_test):\n",
        "    \"\"\"Compute cross entropy loss\"\"\"\n",
        "\n",
        "    y_pred_prob = sigmoid(np.dot(x_test,self._w))\n",
        "    y_pred_prob_m1 = 1 - y_pred_prob\n",
        "\n",
        "    # Replace small values in both with 1e-5 to avoid NAN (log0)\n",
        "    y_pred_prob = np.where(y_pred_prob < 1e-5, 1e-5, y_pred_prob)\n",
        "    y_pred_prob_m1 = np.where(y_pred_prob_m1 < 1e-5, 1e-5, y_pred_prob_m1)\n",
        "\n",
        "    loss_0 = y_test * np.log(y_pred_prob)\n",
        "    loss_1 = (1-y_test) * np.log(y_pred_prob_m1)\n",
        "\n",
        "    loss = -np.sum(loss_0 + loss_1)\n",
        "\n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uovPK-_BTnb"
      },
      "source": [
        "# K-fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64WNG8W6HkjO"
      },
      "source": [
        "def kfold_cross_validation(model, X_train, y_train, k=10, tolerance = 1e-2, standardize_idx=None, printresult=True, learning_rate=\"dependent\"):\n",
        "  \"\"\"Perform k-fold cross validation\"\"\"\n",
        "\n",
        "  # Compute the amount of samples in each fold\n",
        "\n",
        "  fold_size = int(len(X_train)/k) \n",
        "  if printresult:\n",
        "    print (\"Now doing k-fold cross validation, fold size= {}, x train shape = {}, y train shape = {}\".format(fold_size, X_train.shape, y_train.shape))\n",
        "\n",
        "  accuracy_train_store = []\n",
        "  accuracy_test_store = []\n",
        "  iteration_store = []\n",
        "  weight_store_fold = []\n",
        "  fold_CE_store = []\n",
        "  \n",
        "  # Store initial weights to use the same for each fold\n",
        "  initial_weights = model._w\n",
        "\n",
        "  for fold_number in range(k):\n",
        "\n",
        "    # Reset weights\n",
        "    model._w = initial_weights\n",
        "\n",
        "\n",
        "    # Split data\n",
        "    index_start = fold_size*fold_number\n",
        "    index_end = fold_size*(fold_number+1)\n",
        "\n",
        "    if fold_number == (k-1):\n",
        "      # For final fold\n",
        "      X_train_fold = X_train[:index_start,:]\n",
        "      y_train_fold = y_train[:index_start,:]\n",
        "      X_validation_fold = X_train[index_start:,:]\n",
        "      y_validation_fold = y_train[index_start:,:]\n",
        "    \n",
        "    else:\n",
        "      # For all other folds fold\n",
        "      X_train_fold = np.concatenate((X_train[:index_start,:], X_train[index_end:,:]),axis=0)\n",
        "      y_train_fold = np.concatenate((y_train[:index_start,:], y_train[index_end:,:]),axis=0)\n",
        "      X_validation_fold = X_train[index_start:index_end,:]\n",
        "      y_validation_fold = y_train[index_start:index_end,:]\n",
        "\n",
        "\n",
        "    # Standardize if required\n",
        "    X_train_fold_original = X_train_fold.copy()\n",
        "    if standardize_idx == \"all\":\n",
        "      # Standardize all columns\n",
        "      X_train_fold[:,1:] = standardization(X_train_fold[:,1:], X_train_fold_original[:,1:])\n",
        "      X_validation_fold[:,1:] = standardization(X_validation_fold[:,1:], X_train_fold_original[:,1:])\n",
        "    elif standardize_idx is not None:\n",
        "      # Standardize subset of columns\n",
        "      X_train_fold[:, standardize_idx] = standardization(X_train_fold[:, standardize_idx], X_train_fold_original[:,standardize_idx])\n",
        "      X_validation_fold[:, standardize_idx] = standardization(X_validation_fold[:, standardize_idx], X_train_fold_original[:,standardize_idx])\n",
        "    else:\n",
        "      # Do not standardize\n",
        "      pass\n",
        "\n",
        "    # Fit the model\n",
        "    t_start = time.time()\n",
        "    accuracy_train, iteration, weight_store = model.fit(X_train_fold, y_train_fold, max_iters = 15000, tolerance=tolerance, print_results=False, learning_rate=learning_rate)\n",
        "    t_end = time.time()\n",
        "\n",
        "    # Compute accuracy and cross-entropy loss\n",
        "    y_pred_test = model.predict(X_validation_fold)\n",
        "    accuracy_test = model.accu_eval(y_validation_fold, y_pred_test)\n",
        "    cross_entropy = model.cross_entropy_loss(X_validation_fold,y_validation_fold)\n",
        "\n",
        "    # Store values\n",
        "    accuracy_train_store.append(accuracy_train)\n",
        "    accuracy_test_store.append(accuracy_test)\n",
        "    iteration_store.append(iteration)\n",
        "    fold_CE_store.append(cross_entropy)\n",
        "    weight_store_fold.append(weight_store)\n",
        "    \n",
        "    if printresult:\n",
        "      print(f\"### Fold number {fold_number+1} ###\")\n",
        "      print(f\"Execution time: {t_end-t_start:.3f}s\")\n",
        "      print(f\"Training Accuracy: {100*accuracy_train:.2f} %\")\n",
        "      print(f'Testing Accuracy: {100*accuracy_test:.2f} %')\n",
        "      print(f'Cross-entropy loss CE: {cross_entropy}')\n",
        "\n",
        "  cross_accuracy = np.mean(accuracy_test_store)\n",
        "  if printresult:\n",
        "    print(\"###########################\")\n",
        "    print (f\"Mean testing accuracy is {cross_accuracy*100:.2f} %\" )\n",
        "  \n",
        "  return accuracy_train_store, accuracy_test_store, iteration_store, weight_store_fold\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk3dav7RCd3x"
      },
      "source": [
        "# Define a function to easily run experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIbq30TwMbVJ"
      },
      "source": [
        "def test_classifier(model, X, y, num_folds=None, num_loops=1, standardize_idx=None, tolerance=1e-2, max_iters=1000, random_seed=None, print_results=False, learning_rate=\"dependent\"):\n",
        "  \"\"\"Test the logistic regression\"\"\"\n",
        "\n",
        "  if num_folds is None:\n",
        "  # Do not perform k-fold cv\n",
        "\n",
        "    # Shuffle data\n",
        "    if random_seed is not None:\n",
        "      X, y = shuffle_data(X, y, random_seed)\n",
        "\n",
        "    X_train, y_train, X_test, y_test = splitdata(X, y, 0.8)\n",
        "\n",
        "    X_train_original = X_train.copy()\n",
        "    if standardize_idx == \"all\":\n",
        "      # Standardize all columns\n",
        "      X_train = standardization(X_train, X_train_original)\n",
        "      X_test = standardization(X_test, X_train_original)\n",
        "    elif standardize_idx is not None:\n",
        "      # Standardize subset of columns\n",
        "      X_train[:, standardize_idx] = standardization(X_train_original[:, standardize_idx], X_train_original[:,standardize_idx])\n",
        "      X_test[:, standardize_idx] = standardization(X_test[:, standardize_idx], X_train_original[:,standardize_idx])\n",
        "    else:\n",
        "      # Do not standardize\n",
        "      pass\n",
        "\n",
        "    accuracy_train, iterations, weight_store = model.fit(X_train, y_train, max_iters, tolerance, print_results)\n",
        "    accuracy_test = model.accu_eval(y_test, model.predict(X_test))\n",
        "  else:\n",
        "    # Perform k-fold cross-validation\n",
        "    iterations = []\n",
        "    accuracy_train = []\n",
        "    accuracy_test = []\n",
        "    weight_store = []\n",
        "    for i in range(num_loops):\n",
        "\n",
        "      if random_seed is not None:\n",
        "        X, y = shuffle_data(X, y, random_seed*(i+1))\n",
        "        \n",
        "      accuracy_train_iter, accuracy_test_iter,  iterations_iter, weight_store_iter = \\\n",
        "      kfold_cross_validation(model, X, y, num_folds, tolerance, standardize_idx=standardize_idx, printresult=print_results, learning_rate=learning_rate)\n",
        "\n",
        "      accuracy_train.append(accuracy_train_iter)\n",
        "      accuracy_test.append(accuracy_test_iter)\n",
        "      iterations.append(iterations_iter)\n",
        "      weight_store.append(weight_store_iter)\n",
        "\n",
        "  return accuracy_train, accuracy_test, iterations, weight_store"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1-DPMDysD7e"
      },
      "source": [
        "# Set baseline accuracy and iterations for hepatitis data\n",
        "- Initial weights: zeros\n",
        "- Learning rate = 1/1+k\n",
        "- Stopping criteria: epsilon = 1e-2\n",
        "- No standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwapfUW_yot5"
      },
      "source": [
        "# RIGHT NOW, BASELINE IS DEFINED AS THE RESULT OF\n",
        "# (1)ONE RANDOM 10-FOLD CV, (2)NO STANDARDIZATION, (3)WITH SHUFFLE (random_seed)\n",
        "\n",
        "# Set random seed, num_fold, num_loops\n",
        "seed = 0\n",
        "fold = 10\n",
        "loops = 10\n",
        "\n",
        "# Create original set of features\n",
        "X_hep_original_no_bias = hep_data.iloc[:, :-1].to_numpy()\n",
        "X_hep_original = np.insert(\n",
        "    X_hep_original_no_bias, 0, np.ones(X_hep_original_no_bias.shape[0]), axis=1\n",
        ")\n",
        "y_hep_original = hep_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
        "\n",
        "# Account for extra column for bias\n",
        "index_num_columns = index_num_columns_raw + 1\n",
        "index_cat_columns = index_cat_columns_raw + 1\n",
        "\n",
        "# Create an instance of the model object\n",
        "initial_weights = np.zeros((X_hep_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_original, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed,\n",
        "                                                            learning_rate=\"dependent\")\n",
        "\n",
        "baseline_accuracy_hep = np.mean(accuracy_test)\n",
        "baseline_iterations_hep = np.mean(iterations)\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*baseline_accuracy_hep:.2f} %\")\n",
        "print(f\"Number of iterations: {baseline_iterations_hep}\")\n",
        "\n",
        "plt.title('k-fold accuracy')\n",
        "plt.xlabel('fold number'), plt.ylabel('accuracy')\n",
        "plt.plot(accuracy_test[0])\n",
        "plt.show()\n",
        "\n",
        "plt.title('k-fold iteration')\n",
        "plt.xlabel('fold number'), plt.ylabel('iteration')\n",
        "plt.plot(iterations[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQDzGkMdiezu"
      },
      "source": [
        "# Hepatitis Test 1: Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFVBV-nMimNx"
      },
      "source": [
        "# Create an instance of the model object\n",
        "initial_weights = np.zeros((X_hep_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_original, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=index_num_columns,\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_hep:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_hep}\")\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TPR-c8wnbpS"
      },
      "source": [
        "# Hepatitis Test 2: Initial Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqtHW9xvnjcC"
      },
      "source": [
        "# Create an instance of the model object, weights set between 0 and 1\n",
        "initial_weights = np.random.rand(X_hep_original.shape[1],1)\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_original, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_hep:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_hep}\")\n",
        "\n",
        "print(\"#### Test group 1: Initial weight random 0 to 1 ####\")\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\")\n",
        "\n",
        "\n",
        "\n",
        "# Create an instance of the model object, weights set between 0 and 1\n",
        "initial_weights = np.random.rand(X_hep_original.shape[1],1) * 100\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_original, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(\"#### Test group 2: Initial weight random 0 to 100 ####\")\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\")\n",
        "\n",
        "# Create an instance of the model object, weights set between 0 and 1\n",
        "initial_weights = np.random.randn(X_hep_original.shape[1],1)\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_original, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(\"#### Test group 3: Initial weight drawn from standard normal ####\")\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOn7yEohpzrQ"
      },
      "source": [
        "# Hepatitis Test 3: Stopping Condition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hEjh3VaqAiu"
      },
      "source": [
        "# Create an instance of the model object, weights set between 0 and 1\n",
        "initial_weights =  np.zeros((X_hep_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_original, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed,\n",
        "                                                            tolerance=1e-6)\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_hep:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_hep}\")\n",
        "\n",
        "print(\"#### Test group 1: Tolerance 1e-6 ####\")\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\")\n",
        "\n",
        "\n",
        "# Create an instance of the model object, weights set between 0 and 1\n",
        "initial_weights =  np.zeros((X_hep_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store  = test_classifier(model, X_hep_original, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed,\n",
        "                                                            tolerance=1e-9)\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_hep:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_hep}\")\n",
        "\n",
        "print(\"#### Test group 2: Tolerance 1e-9 ####\")\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd0f4h7geEoa"
      },
      "source": [
        "# Hepatitis Test 4: Removing independent features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkCWAOMEeBcB"
      },
      "source": [
        "# indices of independent features to remove\n",
        "features_to_remove = [2, 3, 4, 8, 9, 16]\n",
        "initial_weights = np.zeros((X_hep_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "Improve_list_removing = []\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_original, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed,)\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_hep:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_hep}\")\n",
        "\n",
        "for i in features_to_remove:\n",
        "  print(f\"Removing {hep_data.columns[i-1]}...\")\n",
        "  X_hep_remove = np.delete(X_hep_original, i, axis = 1)\n",
        "\n",
        "  initial_weights = np.zeros((X_hep_remove.shape[1], 1))\n",
        "  model = LogisticClassifier(initial_weights)\n",
        "\n",
        "  accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_remove, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed,)\n",
        "  print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "  print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "  print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "  if (np.mean(accuracy_test)>baseline_accuracy_hep):\n",
        "    Improve_list_removing.append(i)\n",
        "\n",
        "# Remove all independent features\n",
        "print(f\"Removing all independent features...\")\n",
        "X_hep_remove = np.delete(X_hep_original, features_to_remove, axis=1)\n",
        "\n",
        "initial_weights = np.zeros((X_hep_remove.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_remove, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed,)\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "\n",
        "# Remove accu-improved independent features\n",
        "print(f\"Removing all independent features...\")\n",
        "print (\"List of removing independent feature which accuracy improve\",Improve_list_removing)\n",
        "X_hep_remove = np.delete(X_hep_original, Improve_list_removing, axis=1)\n",
        "\n",
        "initial_weights = np.zeros((X_hep_remove.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_remove, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed,)\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "print(f\"Training accuracy improve: {100*(np.mean(accuracy_test)-baseline_accuracy_hep):.2f} %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1eEiqKrXUjh"
      },
      "source": [
        "# Hepatitis Test 5: Log transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgSLRkkxXaYt"
      },
      "source": [
        "feature_to_test = index_num_columns\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_hep:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_hep}\")\n",
        "\n",
        "\n",
        "\n",
        "# Test 1: replace selected features with log, no standardization\n",
        "print(f\"----------------------------------------------\")\n",
        "print(f\"Test 1: replace selected features with log, w/o standardization\")\n",
        "X_hep_log = X_hep_original.copy()\n",
        "X_hep_log_1 = log_transform(X_hep_log, feature_to_test)\n",
        "initial_weights = np.zeros((X_hep_log_1.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_log_1, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,                                                            \n",
        "                                                            random_seed=seed,)\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "print(f\"Training accuracy improve: {100*(np.mean(accuracy_test)-baseline_accuracy_hep):.2f} %\")\n",
        "\n",
        "# Test 2: replace selected features with log, no standardization\n",
        "print(f\"----------------------------------------------\")\n",
        "print(f\"Test 2: replace selected features with log, w/ standardization\")\n",
        "X_hep_log = X_hep_original.copy()\n",
        "X_hep_log_2 = log_transform(X_hep_log, feature_to_test)\n",
        "initial_weights = np.zeros((X_hep_log_2.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_log_2, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,                                                             \n",
        "                                                            standardize_idx=index_num_columns,\n",
        "                                                            random_seed=seed,)\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "print(f\"Training accuracy improve: {100*(np.mean(accuracy_test)-baseline_accuracy_hep):.2f} %\")\n",
        "\n",
        "# Test 3: append selected features with log, no standardization\n",
        "print(f\"----------------------------------------------\")\n",
        "print(f\"Test 3: append log transform of selected features, w/o standardization\")\n",
        "X_hep_log = X_hep_original.copy()\n",
        "X_hep_log_3 = log_transform(X_hep_log, feature_to_test, replace=False)\n",
        "initial_weights = np.zeros((X_hep_log_3.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_log_3, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,                                                            \n",
        "                                                            random_seed=seed,)\n",
        "\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "print(f\"Training accuracy improve: {100*(np.mean(accuracy_test)-baseline_accuracy_hep):.2f} %\")\n",
        "\n",
        "# Test 4: append selected features with log, with standardization\n",
        "print(f\"----------------------------------------------\")\n",
        "print(f\"Test 4: append log transform of selected features, w standardization\")\n",
        "X_hep_log = X_hep_original.copy()\n",
        "X_hep_log_4 = log_transform(X_hep_log, feature_to_test, replace=False)\n",
        "initial_weights = np.zeros((X_hep_log_4.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# TODO Need to add new features to standardization\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_log_4, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,                                                     \n",
        "                                                            standardize_idx=index_num_columns,\n",
        "                                                            random_seed=seed,)\n",
        "\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "print(f\"Training accuracy improve: {100*(np.mean(accuracy_test)-baseline_accuracy_hep):.2f} %\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyw5TAEyQSOZ"
      },
      "source": [
        "# Hepatitis Test 6: Final results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkLJi8lZJHPM"
      },
      "source": [
        "Test (Anne 10/11)\n",
        "1. Log append only, init 0-1, +1% (Best??)\n",
        "2. Delete only, init 0-1, +0.5%\n",
        "3. Delete+log append, init 0-1, +0.7%\n",
        "4. Log append only, init 0s, +0.5%\n",
        "5. Delete only, init 0s, +0.51%\n",
        "6. Delete+log append, init 0s, +0.48%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWLmYy1CQRp7"
      },
      "source": [
        "#feature_to_test = index_num_columns\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_hep:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_hep}\")\n",
        "\n",
        "X_hep_final = X_hep_original.copy()\n",
        "\n",
        "# Remove features \n",
        "X_hep_final = np.delete(X_hep_final, [2, 3, 9], axis=1)\n",
        "\n",
        "# New locations of numerical features\n",
        "index_num_columns_new = np.array([1, 11, 12, 13, 14, 15])\n",
        "\n",
        "# X_hep_final = log_transform(X_hep_final, index_num_columns_new, replace=False)\n",
        "feature_to_test = index_num_columns_new.copy()\n",
        "X_hep_final = log_transform(X_hep_final, feature_to_test, replace=False)\n",
        "#X_hep_final = log_transform(X_hep_final, feature_to_test)\n",
        "\n",
        "initial_weights = np.random.rand(X_hep_final.shape[1],1)\n",
        "#initial_weights = np.zeros((X_hep_final.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_hep_final, \n",
        "                                                            y_hep_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,                                                            \n",
        "                                                            random_seed=seed,\n",
        "                                                            tolerance=1e-2)\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "print(f\"Training accuracy improve: {100*(np.mean(accuracy_test)-baseline_accuracy_hep):.2f} %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i35_4yYMcASS"
      },
      "source": [
        "# Set baseline accuracy and iterations for bankruptcy data\n",
        "- Initial weights: zeros\n",
        "- Learning rate = 1/1+k\n",
        "- Stopping criteria: epsilon = 1e-2\n",
        "- No standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjdw7mgVTG7i"
      },
      "source": [
        "# RIGHT NOW, BASELINE IS DEFINED AS THE RESULT OF\n",
        "# (1)ONE RANDOM 10-FOLD CV, (2)NO STANDARDIZATION, (3)WITH SHUFFLE (random_seed)\n",
        "\n",
        "# Set random seed, num_fold, num_loops\n",
        "seed = 10\n",
        "fold = 10\n",
        "loops = 1\n",
        "\n",
        "# Create original set of features\n",
        "X_bank_original_no_bias = bank_data.iloc[:, :-1].to_numpy()\n",
        "X_bank_original = np.insert(\n",
        "    X_bank_original_no_bias, 0, np.ones(X_bank_original_no_bias.shape[0]), axis=1\n",
        ")\n",
        "y_bank_original = bank_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Create an instance of the model object\n",
        "initial_weights = np.zeros((X_bank_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_original, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "baseline_accuracy_bank = np.mean(accuracy_test)\n",
        "baseline_iterations_bank = np.mean(iterations)\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*baseline_accuracy_bank:.2f} %\")\n",
        "print(f\"Number of iterations: {baseline_iterations_bank}\")\n",
        "\n",
        "plt.title('k-fold accuracy')\n",
        "plt.xlabel('fold number'), plt.ylabel('accuracy')\n",
        "plt.plot(accuracy_test[0])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.title('k-fold iteration')\n",
        "plt.xlabel('fold number'), plt.ylabel('iteration')\n",
        "plt.plot(iterations[0])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn0xVMl4X12H"
      },
      "source": [
        "# Bankruptcy Test 1: Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_uSnwJClQOP"
      },
      "source": [
        "# Create an instance of the model object\n",
        "initial_weights = np.zeros((X_bank_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_original, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=\"all\",\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_bank:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_bank}\")\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n18U-YzXj4P4"
      },
      "source": [
        "# Bankruptcy Test 2: Initial Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X82Y-oWqkUDB"
      },
      "source": [
        "# Create an instance of the model object, weights set between 0 and 1\n",
        "initial_weights = np.random.rand(X_bank_original.shape[1],1)\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_original, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_bank:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_bank}\")\n",
        "\n",
        "print(\"#### Test group 1: Initial weight random 0 to 1 ####\")\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\")\n",
        "\n",
        "\n",
        "\n",
        "# Create an instance of the model object, weights set between 0 and 1\n",
        "initial_weights = np.random.rand(X_bank_original.shape[1],1) * 100\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_original, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(\"#### Test group 2: Initial weight random 0 to 100 ####\")\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\")\n",
        "\n",
        "# Create an instance of the model object, weights set between 0 and 1\n",
        "initial_weights = np.random.randn(X_bank_original.shape[1],1)\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_original, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(\"#### Test group 3: Initial weight drawn from standard normal ####\")\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP4mlkNslF4b"
      },
      "source": [
        "# Bankruptcy Test 3: Stopping Condition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXI93ViPlGzq"
      },
      "source": [
        "# Create an instance of the model object, weights set between 0 and 1\n",
        "initial_weights =  np.zeros((X_bank_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_original, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed,\n",
        "                                                            tolerance=1e-6)\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_bank:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_bank}\")\n",
        "\n",
        "print(\"#### Test group 1: Tolerance 1e-6 ####\")\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\")\n",
        "\n",
        "\n",
        "# Create an instance of the model object, weights set between 0 and 1\n",
        "initial_weights =  np.zeros((X_bank_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store  = test_classifier(model, X_bank_original, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed,\n",
        "                                                            tolerance=1e-9)\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_bank:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_bank}\")\n",
        "\n",
        "print(\"#### Test group 2: Tolerance 1e-9 ####\")\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdFI-_e3I_8x"
      },
      "source": [
        "# Bankruptcy Test 4: Removing independent features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc_5waNyJKnx"
      },
      "source": [
        "# generate indices of independent features to remove\n",
        "Threshold = 0.15\n",
        "correlation = bank_data.corr()\n",
        "correlation_arr = np.abs(correlation.to_numpy())\n",
        "features_to_remove = []\n",
        "count = 0\n",
        "for i in (correlation_arr[:,-1]):\n",
        "  if (i<Threshold):\n",
        "    features_to_remove.append(count)\n",
        "  count +=1\n",
        "\n",
        "\n",
        "initial_weights = np.zeros((X_bank_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "Improve_list_removing = []\n",
        "\n",
        "# Run one k-fold cross validation\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_original, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,                                                          \n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_bank:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_bank}\")\n",
        "\n",
        "for i in features_to_remove:\n",
        "  print(f\"Removing {bank_data.columns[i-1]}...\")\n",
        "  X_bank_remove = np.delete(X_bank_original, i, axis = 1)\n",
        "\n",
        "  initial_weights = np.zeros((X_bank_remove.shape[1], 1))\n",
        "  model = LogisticClassifier(initial_weights)\n",
        "\n",
        "  accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_remove, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,                                                            \n",
        "                                                            random_seed=seed,)\n",
        "  print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "  print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "  print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "  if (np.mean(accuracy_test)>baseline_accuracy_bank):\n",
        "    Improve_list_removing.append(i)\n",
        "\n",
        "# Remove all independent features\n",
        "print(f\"Removing all independent features...\")\n",
        "X_bank_remove = np.delete(X_bank_original, features_to_remove, axis=1)\n",
        "\n",
        "initial_weights = np.zeros((X_bank_remove.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_remove, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,                                                            \n",
        "                                                            random_seed=seed,)\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "\n",
        "# Remove accu-improved independent features\n",
        "print (\"List of removing independent feature which accuracy improve\",Improve_list_removing)\n",
        "print (len(Improve_list_removing))\n",
        "X_bank_remove = np.delete(X_bank_original, Improve_list_removing, axis=1)\n",
        "\n",
        "initial_weights = np.zeros((X_bank_remove.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_remove, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=None,                                                            \n",
        "                                                            random_seed=seed,)\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "print(f\"Training accuracy improve: {100*(np.mean(accuracy_test)-baseline_accuracy_bank):.2f} %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9YliBO3phDI"
      },
      "source": [
        "# Bankruptcy Test 5: Log transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3jDfr2dqWDd"
      },
      "source": [
        "##########Test block############\n",
        "\n",
        "print(X_bank_original)\n",
        "print(X_bank_original.shape)\n",
        "feature_to_test = range(1,X_bank_log.shape[1]-1)\n",
        "print(feature_to_test)\n",
        "\n",
        "# shift all values up to remove negative values\n",
        "X_bank_norm = X_bank_original.copy()\n",
        "for i in feature_to_test:\n",
        "  X_bank_norm[:,i] = X_bank_norm[:,i] + np.absolute(np.min(X_bank_norm[:,i]))\n",
        "\n",
        "# Test 1: replace selected features with log, no standardization\n",
        "print(f\"----------------------------------------------\")\n",
        "print(f\"Test 1: replace selected features with log, w/o standardization\")\n",
        "X_bank_log = X_bank_norm.copy()\n",
        "X_bank_log_1 = log_transform(X_bank_log, feature_to_test, replace=False, bank=True)\n",
        "\n",
        "print(X_bank_log_1.shape)\n",
        "print(X_bank_log_1[:, 80])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7Xy8WWrpmt1"
      },
      "source": [
        "# Set random seed, num_fold, num_loops\n",
        "seed = 10\n",
        "fold = 10\n",
        "loops = 1\n",
        "\n",
        "# Create original set of features\n",
        "X_bank_original_no_bias = bank_data.iloc[:, :-1].to_numpy()\n",
        "X_bank_original = np.insert(\n",
        "    X_bank_original_no_bias, 0, np.ones(X_bank_original_no_bias.shape[0]), axis=1\n",
        ")\n",
        "y_bank_original = bank_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
        "\n",
        "\n",
        "feature_to_test = range(1,X_bank_original.shape[1]-1)\n",
        "\n",
        "# Baseline result\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_bank:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_bank}\")\n",
        "\n",
        "# shift all values up to remove negative values\n",
        "#X_bank_norm = X_bank_original.copy()\n",
        "#for i in feature_to_test:\n",
        "#  X_bank_norm[:,i] = X_bank_norm[:,i] + np.absolute(np.min(X_bank_norm[:,i]))\n",
        "\n",
        "# Test 1: replace selected features with log, no standardization\n",
        "print(f\"----------------------------------------------\")\n",
        "print(f\"Test 1: replace selected features with log, w/o standardization\")\n",
        "X_bank_log = X_bank_original.copy()\n",
        "X_bank_log_1 = log_transform(X_bank_log, feature_to_test, bank=True)\n",
        "initial_weights = np.zeros((X_bank_log_1.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_log_1, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,\n",
        "                                                            tolerance=1e-6,\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "print(f\"Training accuracy improve: {100*(np.mean(accuracy_test)-baseline_accuracy_bank):.2f} %\")\n",
        "\n",
        "# Test 2: replace selected features with log, no standardization\n",
        "print(f\"----------------------------------------------\")\n",
        "print(f\"Test 2: replace selected features with log, w/ standardization\")\n",
        "X_bank_log = X_bank_original.copy()\n",
        "X_bank_log_2 = log_transform(X_bank_log, feature_to_test, bank=True)\n",
        "initial_weights = np.zeros((X_bank_log_2.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_log_2, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=\"all\",\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "print(f\"Training accuracy improve: {100*(np.mean(accuracy_test)-baseline_accuracy_bank):.2f} %\")\n",
        "\n",
        "# Test 3: replace selected features with log, no standardization\n",
        "print(f\"----------------------------------------------\")\n",
        "print(f\"Test 3: append log transform of selected features, w/o standardization\")\n",
        "X_bank_log = X_bank_original.copy()\n",
        "X_bank_log_3 = log_transform(X_bank_log, feature_to_test, replace=False, bank=True)\n",
        "initial_weights = np.zeros((X_bank_log_3.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_log_3, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "print(f\"Training accuracy improve: {100*(np.mean(accuracy_test)-baseline_accuracy_bank):.2f} %\")\n",
        "\n",
        "# Test 4: replace selected features with log, with standardization\n",
        "print(f\"----------------------------------------------\")\n",
        "print(f\"Test 4: append log transform of selected features, w standardization\")\n",
        "X_bank_log = X_bank_original.copy()\n",
        "X_bank_log_4 = log_transform(X_bank_log, feature_to_test, replace=False, bank=True)\n",
        "initial_weights = np.zeros((X_bank_log_4.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_log_4, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops, \n",
        "                                                            standardize_idx=\"all\",\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "print(f\"Training accuracy improve: {100*(np.mean(accuracy_test)-baseline_accuracy_bank):.2f} %\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAQRYXs6Uoc2"
      },
      "source": [
        "# Bankruptcy Test 6: Final Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "409wxu8XLYZG"
      },
      "source": [
        "Test: (Anne 10/11)\n",
        "1. delete + log append, init 0s, +3.74% (Best??)\n",
        "2. delete only, init 0s, +3.32%\n",
        "3. log append only, init 0s, +1.11%\n",
        "4. delete + log replace, init 0s, +3.71%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9M5WJhOuxs"
      },
      "source": [
        "features_to_remove = np.array([4, 12, 19, 20, 22, 26, 33, 35, 36, 43, 48, 51, 53, 59, 60, 61, 63])\n",
        "\n",
        "# Set random seed, num_fold, num_loops\n",
        "seed = 10\n",
        "fold = 10\n",
        "loops = 1\n",
        "\n",
        "# Baseline result\n",
        "\n",
        "print(f\"Baseline testing accuracy: {100*baseline_accuracy_bank:.2f} %\")\n",
        "print(f\"Baseline iterations: {baseline_iterations_bank}\")\n",
        "\n",
        "X_bank_final = X_bank_original.copy()\n",
        "\n",
        "# Remove features\n",
        "X_bank_final = np.delete(X_bank_final, features_to_remove, axis=1)\n",
        "feature_to_test = range(1,X_bank_final.shape[1]-1)\n",
        "X_bank_final = log_transform(X_bank_final, feature_to_test, replace=False, bank=True)\n",
        "initial_weights = np.zeros((X_bank_final.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "tolerance_final=1e-6\n",
        "\n",
        "accuracy_train, accuracy_test, iterations, weight_store = test_classifier(model, X_bank_final, \n",
        "                                                            y_bank_original, \n",
        "                                                            num_folds=fold, \n",
        "                                                            num_loops=loops,\n",
        "                                                            standardize_idx=None,\n",
        "                                                            tolerance=tolerance_final,\n",
        "                                                            random_seed=seed)\n",
        "\n",
        "print(f\"Training accuracy: {100*np.mean(accuracy_train):.2f} %\")\n",
        "print(f\"Testing accuracy: {100*np.mean(accuracy_test):.2f} %\")\n",
        "print(f\"Number of iterations: {np.mean(iterations)}\") \n",
        "print(f\"Training accuracy improve: {100*(np.mean(accuracy_test)-baseline_accuracy_bank):.2f} %\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMXytBtwVajE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}