{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Miniproject1_combo.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HungYangChang/ECSE551/blob/master/Miniproject1_combo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLdbMJpptgiB"
      },
      "source": [
        "# Import relevant modules\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z22guoEOgiKx"
      },
      "source": [
        "# Read Data Sets (Bankrupcy and Hepatitis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FWa6B074nO4"
      },
      "source": [
        "# Load bankruptcy data\n",
        "url = \"https://raw.githubusercontent.com/jonarsenault/ecse551data/master/bankrupcy.csv\"\n",
        "bank_data = pd.read_csv(url)\n",
        "\n",
        "# Display some of the data\n",
        "print(bank_data.head())\n",
        "\n",
        "# Print size of data\n",
        "bank_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGv3KRgWuMGM"
      },
      "source": [
        "# Load hepatitis data\n",
        "url = \"https://raw.githubusercontent.com/jonarsenault/ecse551data/master/hepatitis.csv\"\n",
        "hep_data = pd.read_csv(url)\n",
        "\n",
        "# Display some of the data\n",
        "print(hep_data.head())\n",
        "\n",
        "# Print size of data\n",
        "hep_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NiqrhcCcNL1"
      },
      "source": [
        "# CHANGE LOG\n",
        "\n",
        "List of changes made to code: \\\\\n",
        "- Standardization was being performed on both the numerical and categorical data for the hepatitis data. I changed it to only the numerical data. \n",
        "- Split data function now works with numpy array as opposed to pandas dataframe.\n",
        "- Now reporting training and testing accuracy for basic fit and test\n",
        "- We only really care about the time it takes to fit the model, so I set the timers to measure that.\n",
        "- Changed the standardization in the k-fold code to allow standardization of certain columns\n",
        "Jon - 10/6\n",
        "- Fixed the CE loss function. Problem was related to passing 0 to the log function. Even if it's technically defined for numbers very close to 0, the computer can't hold numbers that small.\n",
        "- Added a section to try and\n",
        "\n",
        "List of changes by James 10/4\n",
        "- Not sure why you make split data function works with numpy array as opposed to pandas dataframe. But I cominbe taking random set and taking constant set to one function. TODO: make random works with numpy array\n",
        "- Changed shuffle in the k-fold code to allow shuflling of certain row (parameter shuffle = True). #Good, but x_train and y_train need to be suffled at the same time! So I combined them, shuffled, then split\n",
        "- Copy oringial data to data_kfold before shuffling in k-fold CV to prevent disturbing orinigal data\n",
        "- #Question: why -1?  X_hep_cat_change[:, index_cat_columns] = X_hep_cat_change[:, index_cat_columns] - 1  (you can search Question in code) #Answer: I was just trying something! Typically binary variables are (0,1) but here we have (1,2) I was wondering if that made a difference (and it's still an open question)\n",
        "\n",
        "List of changes by Anne 10/5\n",
        "- Deleted irrelevant blocks\n",
        "- Added CE function in Define functions block. But I don't think it works yet!! give me lots of NANs... Anyone help me out?? Thanks!! :p (Fixed: just passing very small values to the log function)\n",
        "- Added CE in k-fold block, if the functions works, this should allow us to print and plot CE of each fold (Done!)\n",
        "- Fixed bankruptcy basic fit! There is a small error in standardization section in k-fold function. It should not include the first column (bias one), so I fixed it, and it works okay now I think..\n",
        "- Added option to NOT standardize in k-fold\n",
        "- Added section to compare effect of standardization. Do you guys think you can work with this as a template to run other tests? There's separate block for each dataset. In each block, it copies the pre-processed data into two groups, we can modify each group with whatever we want to compare. Then run the k-fold loop (or just k-fold?) for each group to compare results.\n",
        "- In order for the k-fold loop to be consice, I wrote an option to print result of each fold or not. (Good)\n",
        "- Question: why is standardization NOT giving better results??? \n",
        "- I'll keep work on it tomorrow! To do: Fix CE, compare initial weight and stopping criteria k-fold, write results in report\n",
        "\n",
        "List of changes by James 10/6 \\\n",
        "Note\n",
        "- Return iteration time at Define funtion in fit. Now showing (1) average accuracy, (2) each accuracy, (3) iteration time after finsihing k-fold CV.\n",
        "- Fix Basic Fit and test for bankruptcy\n",
        "\n",
        "TODO \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxY6ov7qKnLF"
      },
      "source": [
        "# Define functions for linear classifier logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH2Ra3N6Klmd"
      },
      "source": [
        "# JA NOTE: I would combine these into one function, somehow. We can discuss.\n",
        "# James note: combined split data to one funciton\n",
        "\n",
        "def splitdata(data, perc_training, random_flag=True):\n",
        "  \"\"\"Split data into training and testing set using a random division or by taking a constant set\"\"\"\n",
        "  if (random_flag == True):\n",
        "    train_data = data.sample(frac = perc_training, random_state=np.random.RandomState())\n",
        "    test_data = data.drop(train_data.index)\n",
        "    # train_data = np.random.choice([False, True], len(data), replace=False ,p=[perc_training, (1-perc_training)])\n",
        "  else:\n",
        "    num_rows = data.shape[0]\n",
        "    num_rows_train = int(num_rows * perc_training )\n",
        "    num_rows_test = num_rows - num_rows_train\n",
        "    train_data = data[:num_rows_train, :]\n",
        "    test_data = data[:num_rows_test, :]\n",
        "  return train_data, test_data\n",
        "\n",
        "def standardization(data, training_data):\n",
        "  \"\"\"Standardize each column of input data\"\"\"\n",
        "  data_standardized = (data - training_data.mean(axis=0))/training_data.std(axis=0)\n",
        "  return data_standardized\n",
        "\n",
        "def sigmoid(x):\n",
        "  \"\"\"Apply logistic activation function to input\"\"\"\n",
        "  # Logistic activation function\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define fit, predict and accu_eval functions\n",
        "class LogisticClassifier():\n",
        "  \"\"\"Class defining a logistic classifier\"\"\"\n",
        "  #TODO read initial weights (will be discussed)\n",
        "  def __init__(self, weights):\n",
        "    \"\"\"Constructor\"\"\"\n",
        "    self._w = weights\n",
        "  \n",
        "  def fit(self, x_train, y_train, iter_time, epsilon = 1e-6):\n",
        "    \"\"\"Fit a logistic regression model to data\"\"\"\n",
        "\n",
        "    # Lists to store weights and gradient at each iteration\n",
        "    Weight_store = []\n",
        "\n",
        "    # JA NOTE: Why do we not loop until the stopping criteria is reached as; Anne Note: revised oct.5\n",
        "    # opposed to having a break statement later on\n",
        "    iteration = 1\n",
        "    delta_weights = 1e6\n",
        "\n",
        "    while (delta_weights > epsilon) & (iteration < iter_time):\n",
        "      #TODO selection of learning rate will be discussed\n",
        "\n",
        "      # Set learning rate for this iteration\n",
        "      learning_rate = 1/(1 + iteration)\n",
        "\n",
        "      # Store current weights before updating\n",
        "      weights_previous = self._w\n",
        "\n",
        "      # Compute gradient of cross-entropy loss\n",
        "      gradient = np.sum(\n",
        "      x_train * (y_train - sigmoid(np.dot(x_train, weights_previous))), axis=0\n",
        "      ).reshape(-1, 1)\n",
        "\n",
        "      # Update weights\n",
        "      self._w = weights_previous + learning_rate * gradient\n",
        "\n",
        "      # Compute change in weights\n",
        "      delta_weights = np.linalg.norm(self._w - weights_previous) ** 2    \n",
        "      # TODO stopping criteria (choise of epsilon will be discussed)\n",
        "      # TODO Could also try stopping when loss stops decreasing\n",
        "\n",
        "      # Store weight and CE\n",
        "      Weight_store.append(self._w.flatten()) \n",
        "\n",
        "      iteration += 1  \n",
        "\n",
        "    return Weight_store, iteration\n",
        "\n",
        "    if ((iteration+1)==iter_time):\n",
        "      print (\"Job Done, but failed to converge at \", iter_time, \"times iteration\")\n",
        "    else:\n",
        "      print (\"Model converges at\", iteration+1 ,\"times iteration\")\n",
        "\n",
        "  def predict(self, x_test):\n",
        "    \"\"\"Predict the class labels of a given set of samples\"\"\"    \n",
        "    y_pred_prob = sigmoid(np.dot(x_test, self._w))\n",
        "    y_pred = np.where(y_pred_prob < 0.5, 0, 1)\n",
        "    return y_pred\n",
        "  \n",
        "\n",
        "  def accu_eval(self, y_test, y_pred):\n",
        "    \"\"\"Compute accuracy of model\"\"\" \n",
        "    accuracy = np.count_nonzero(y_test == y_pred) / len(y_test)\n",
        "    return accuracy\n",
        "\n",
        "  def cross_entropy_loss(self, x_test, y_test):\n",
        "    \"\"\"Compute cross entropy \"\"\"\n",
        "\n",
        "    y_pred_prob = sigmoid(np.dot(x_test,self._w))\n",
        "    y_pred_prob_m1 = 1 - y_pred_prob\n",
        "\n",
        "    # Replace small values in both with 1e-5 to avoid NAN (log0)\n",
        "    y_pred_prob = np.where(y_pred_prob < 1e-5, 1e-5, y_pred_prob)\n",
        "    y_pred_prob_m1 = np.where(y_pred_prob_m1 < 1e-5, 1e-5, y_pred_prob_m1)\n",
        "\n",
        "    loss_0 = y_test * np.log(y_pred_prob)\n",
        "    loss_1 = (1-y_test) * np.log(y_pred_prob_m1)\n",
        "\n",
        "    loss = -np.sum(loss_0 + loss_1)\n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uovPK-_BTnb"
      },
      "source": [
        "# K-fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64WNG8W6HkjO"
      },
      "source": [
        "# JA NOTE : I think this function should output a list of the accuracy and CE loss for every fold\n",
        "\n",
        "def k_fold_Cross(test, x_train, y_train, k=10, standardize=True, index_to_standardize=None, shuffle=True, printresult=True):\n",
        "  \"\"\" k-fold cross validation to estimate performance with accuracy\"\"\"\n",
        "\n",
        "  # Shuffle data to introcude randomness to trials\n",
        "  if (shuffle==True):\n",
        "    full_array = np.concatenate((x_train, y_train), axis=1)\n",
        "    np.random.shuffle(full_array)\n",
        "    x_train = full_array[:,:-1]\n",
        "    y_train = full_array[:,[-1]]\n",
        "\n",
        "  #int() is floor funcion. If it's not aliquot, just insert all of remaining data to last fold\n",
        "  fold_size = int(len(x_train)/k) \n",
        "  if (printresult==True):\n",
        "    print (\"Now doing k-fold cross validation, fold size= {}, x train shape = {}, y train shape = {}\".format(fold_size, x_train.shape, y_train.shape))\n",
        "\n",
        "  fold_accuracy_store = []\n",
        "  Weight_store_fold = []\n",
        "  fold_CE_store = []\n",
        "  iteration_store_fold= []\n",
        "\n",
        "  for fold_number in range(k):\n",
        "    index_start = fold_size*fold_number\n",
        "    index_end = fold_size*(fold_number+1)\n",
        "\n",
        "    # for last fold (fold number k)\n",
        "    if (fold_number == (k-1)):\n",
        "      x_train_fold = x_train[:index_start,:]\n",
        "      y_train_fold = y_train[:index_start,:]\n",
        "      x_validation_fold = x_train[index_start:,:]\n",
        "      y_validation_fold = y_train[index_start:,:]\n",
        "    # for non-last fold\n",
        "    else:\n",
        "      x_train_fold = np.concatenate((x_train[:index_start,:], x_train[index_end:,:]),axis=0)\n",
        "      y_train_fold = np.concatenate((y_train[:index_start,:], y_train[index_end:,:]),axis=0)\n",
        "      x_validation_fold = x_train[index_start:index_end,:]\n",
        "      y_validation_fold = y_train[index_start:index_end,:]\n",
        "\n",
        "    # NOT standardize the data\n",
        "    if ~standardize:\n",
        "      x_train_fold_standardized = x_train_fold.copy()\n",
        "      x_validation_fold_standardized = x_validation_fold.copy()\n",
        "\n",
        "    # Standardize the data\n",
        "    if (index_to_standardize is not None) and standardize:\n",
        "      x_train_fold_standardized = x_train_fold.copy()\n",
        "      x_train_fold_standardized[:,index_to_standardize] = standardization(\n",
        "          x_train_fold[:,index_to_standardize], \n",
        "          x_train_fold[:,index_to_standardize]\n",
        "      )\n",
        "      x_validation_fold_standardized = x_validation_fold.copy()\n",
        "      x_validation_fold_standardized[:,index_to_standardize] = standardization(\n",
        "          x_validation_fold[:,index_to_standardize], \n",
        "          x_train_fold[:,index_to_standardize]\n",
        "      )\n",
        "      #print('section 1 ran')\n",
        "    elif standardize==True:\n",
        "      x_train_fold_standardized = x_train_fold.copy()\n",
        "      x_train_fold_standardized[:,1:] = standardization(x_train_fold[:,1:], x_train_fold[:,1:])\n",
        "      x_validation_fold_standardized = x_validation_fold.copy()\n",
        "      x_validation_fold_standardized[:,1:] = standardization(x_validation_fold[:,1:], x_train_fold[:,1:])\n",
        "      #print('section 2 ran')\n",
        "\n",
        "    # Fit the model\n",
        "    t_start = time.time()\n",
        "    weights, iteration = test.fit(x_train_fold_standardized, y_train_fold, iter_time = 10000)\n",
        "    t_end = time.time()\n",
        "\n",
        "    Weight_store_fold.append(weights)\n",
        "    iteration_store_fold.append(iteration)\n",
        "    y_pred_train = test.predict(x_train_fold_standardized)\n",
        "    y_pred_test = test.predict(x_validation_fold_standardized)\n",
        "    accuracy_train = test.accu_eval(y_train_fold, y_pred_train)\n",
        "    accuracy_test = test.accu_eval(y_validation_fold, y_pred_test)\n",
        "    fold_accuracy_store.append(accuracy_test)\n",
        "    \n",
        "    cross_E=test.cross_entropy_loss(x_validation_fold_standardized,y_validation_fold)\n",
        "\n",
        "    fold_CE_store.append(cross_E)\n",
        "     \n",
        "    if (printresult==True):\n",
        "      print(f\"### Fold number {fold_number+1} ###\")\n",
        "      print(f\"Execution time: {t_end-t_start:.3f}s\")\n",
        "      print(f\"Training Accuracy: {100*accuracy_train:.2f} %\")\n",
        "      print(f'Testing Accuracy: {100*accuracy_test:.2f} %')\n",
        "      print(f'Cross-entropy loss CE: {cross_E}')\n",
        "\n",
        "  cross_accuracy = sum(fold_accuracy_store)/k\n",
        "  if (printresult==True):\n",
        "    print (f\"Mean testing accuracy is {cross_accuracy*100:.2f} %\" )\n",
        "  \n",
        "  return fold_accuracy_store, Weight_store_fold, fold_CE_store, iteration_store_fold\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1-DPMDysD7e"
      },
      "source": [
        "# Basic fit and test of Hepatitis data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwapfUW_yot5"
      },
      "source": [
        "# Indices of features\n",
        "index_cat_columns = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18])\n",
        "index_numerical_columns = np.array([0, 13, 14, 15, 16, 17])\n",
        "\n",
        "# Create original set of features\n",
        "X_hep_original_no_bias = hep_data.iloc[:, :-1].to_numpy()\n",
        "X_hep_original = np.insert(\n",
        "    X_hep_original_no_bias, 0, np.ones(X_hep_original_no_bias.shape[0]), axis=1\n",
        ")\n",
        "y_hep_original = hep_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
        "\n",
        "# Account for extra column for bias\n",
        "index_numerical_columns = index_numerical_columns + 1\n",
        "index_cat_columns = index_cat_columns + 1\n",
        "\n",
        "# Create an instance of the model object\n",
        "initial_weights = np.zeros((X_hep_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test = splitdata(X_hep_original, 0.8, False)\n",
        "y_train, y_test = splitdata(y_hep_original, 0.8, False)\n",
        "\n",
        "# Standardize the data\n",
        "X_train_standardize = X_train.copy()\n",
        "X_test_standardize = X_test.copy()\n",
        "\n",
        "X_train_standardize[:, index_numerical_columns] = standardization(\n",
        "    X_train[:, index_numerical_columns], X_train[:, index_numerical_columns]\n",
        ")\n",
        "X_test_standardize[:, index_numerical_columns] = standardization(\n",
        "    X_test[:, index_numerical_columns], X_train[:, index_numerical_columns]\n",
        ")\n",
        "\n",
        "# X_train_standardize[:, index_cat_columns] = X_train_standardize[:, index_cat_columns] - 1\n",
        "# X_test_standardize[:, index_cat_columns] = X_test_standardize[:, index_cat_columns] - 1\n",
        "\n",
        "# Question: do we need to standardize y_train or y_test?\n",
        "# Fit the model\n",
        "t_start = time.time() # Start the time\n",
        "model.fit(X_train_standardize, y_train, iter_time=1000)\n",
        "t_end = time.time() # End the timer\n",
        "\n",
        "# Compute accuracy on testing set\n",
        "y_pred_train = model.predict(X_train_standardize)\n",
        "y_pred_test = model.predict(X_test_standardize)\n",
        "\n",
        "accuracy_test = model.accu_eval(y_test, y_pred_test)\n",
        "accuracy_train = model.accu_eval(y_train, y_pred_train)\n",
        "\n",
        "print(f\"Training accuracy: {100*accuracy_train:.2f}\")\n",
        "print(f\"Testing accuracy: {100*accuracy_test:.2f}\")\n",
        "print(f\"Executation time: {(t_end - t_start):.3f}s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFypKWCvg45E"
      },
      "source": [
        "# Implement k-fold cross validation\n",
        "\n",
        "# Create an instance of the model object\n",
        "initial_weights = np.zeros((X_hep_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Question: Why doing this?  OK\n",
        "X_hep_cat_change = X_hep_original.copy()\n",
        "X_hep_cat_change[:, index_cat_columns] = X_hep_cat_change[:, index_cat_columns] - 1 \n",
        "\n",
        "## Shuffling function will shuffle X&Y_hep_original, copy for kfold CV to use\n",
        "X_hep_cat_kfold = X_hep_original.copy()\n",
        "y_hep_cat_kfold = y_hep_original.copy()\n",
        "\n",
        "# Perform cross-validation\n",
        "cross_accuracy_list, weight_fold_list, CE_list, iteration_fold_list  = k_fold_Cross(model, X_hep_cat_kfold,\n",
        "                                                     y_hep_cat_kfold, k=5, \n",
        "                                                     index_to_standardize=index_numerical_columns,\n",
        "                                                     shuffle=True, printresult=True)\n",
        "\n",
        "# print(f\"It costs {(t_end-tStart)} seconds to finish k-fold cross validation\")\n",
        "plt.title('k-fold accuracy')\n",
        "plt.xlabel('fold number'), plt.ylabel('accuracy')\n",
        "plt.plot(cross_accuracy_list)\n",
        "plt.show()\n",
        "\n",
        "plt.title('k-fold cross entropy loss')\n",
        "plt.xlabel('fold number'), plt.ylabel('CE')\n",
        "plt.plot(CE_list)\n",
        "plt.show()\n",
        "\"\"\"\n",
        "plt.title('k-fold iteration')\n",
        "plt.xlabel('fold number'), plt.ylabel('iteration')\n",
        "plt.plot(iteration_fold_list)\n",
        "plt.show()\n",
        "\"\"\"\n",
        "print (\"iteration time\", iteration_fold_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-j0uO--UW2j"
      },
      "source": [
        "# Loop K-fold\n",
        "k = 5\n",
        "loop = 10\n",
        "cross_accuracy = 0\n",
        "iteration_fold = []\n",
        "accuracy_fold = []\n",
        "for i in range(loop):\n",
        "  cross_accuracy_list, weight_fold_list, CE_list, iteration_fold_list = k_fold_Cross(model, X_hep_cat_kfold,\n",
        "                                                     y_hep_cat_kfold, k=5, \n",
        "                                                     index_to_standardize=index_numerical_columns,\n",
        "                                                     shuffle=True, printresult=False)\n",
        "  cross_accuracy += sum(cross_accuracy_list)/k\n",
        "  accuracy_fold.append(np.around((sum(cross_accuracy_list)/k)*100,2))\n",
        "  iteration_fold.append(sum(iteration_fold_list)/k)\n",
        "  \n",
        "\n",
        "cross_accuracy = cross_accuracy/loop\n",
        "\n",
        "print(f'Average cross accuracy is {100*cross_accuracy:.2f} %')\n",
        "print (accuracy_fold)\n",
        "print (iteration_fold)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSztousGe6Xk"
      },
      "source": [
        "# Basic fit and test of Bankruptcy data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjdw7mgVTG7i"
      },
      "source": [
        "# split data into training and test sets\n",
        "# !!!only need to to change data name here\n",
        "\"\"\"\n",
        "data_train, data_test = splitdata(bank_data, 0.8, True)\n",
        "\n",
        "# pre-process trainding set (adding column 0 and standardization)\n",
        "X_without_intercept = data_train.iloc[:,:-1].to_numpy() #all rows all columns except last column\n",
        "x_train_pre = np.insert(X_without_intercept, 0, np.ones(X_without_intercept.shape[0]),\n",
        "              axis=1)\n",
        "x_train = x_train_pre\n",
        "x_train[:, 1:] = standardization(x_train[:,1:],x_train_pre[:,1:])\n",
        "y_train = data_train.iloc[:,-1].to_numpy().reshape(-1,1)\n",
        "\n",
        "# predict with test set\n",
        "X_without_intercept_test = data_test.iloc[:,:-1].to_numpy()\n",
        "x_test = np.insert(X_without_intercept_test, 0, np.ones(X_without_intercept_test.shape[0]),\n",
        "              axis=1)\n",
        "x_test[:, 1:] = standardization(x_test[:,1:],x_train_pre[:,1:])\n",
        "y_test = data_test.iloc[:,-1].to_numpy().reshape(-1,1)\n",
        "w_init = np.zeros((x_train.shape[1], 1))\n",
        "test = LogisticClassifier(w_init)\n",
        "\"\"\"\n",
        "\n",
        "X_bank_original_no_bias = bank_data.iloc[:, :-1].to_numpy()\n",
        "X_bank_original = np.insert(\n",
        "    X_bank_original_no_bias, 0, np.ones(X_bank_original_no_bias.shape[0]), axis=1\n",
        ")\n",
        "y_bank_original = bank_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
        "\n",
        "X_train, X_test = splitdata(X_bank_original, 0.8, False)\n",
        "y_train, y_test = splitdata(y_bank_original, 0.8, False)\n",
        "\n",
        "# Create an instance of the model object\n",
        "initial_weights = np.zeros((X_train.shape[1], 1))\n",
        "test = LogisticClassifier(initial_weights)\n",
        "\n",
        "# Fit the model\n",
        "t_start = time.time() # Start the time\n",
        "Weight_store = test.fit(X_train, y_train, iter_time = 10000)\n",
        "t_end = time.time() # End the timer\n",
        "\n",
        "# Compute accuracy on testing set\n",
        "y_pred_train = test.predict(X_train)\n",
        "y_pred_test = test.predict(X_test)\n",
        "\n",
        "accuracy_test = test.accu_eval(y_test, y_pred_test)\n",
        "accuracy_train = test.accu_eval(y_train, y_pred_train)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Training accuracy: {100*accuracy_train:.2f}\")\n",
        "print(f\"Testing accuracy: {100*accuracy_test:.2f}\")\n",
        "print(f\"Executation time: {(t_end - t_start):.3f}s\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy3HVW2hXM99"
      },
      "source": [
        "# Implement k-fold cross validation\n",
        "X_bank_original_no_bias = bank_data.iloc[:, :-1].to_numpy()\n",
        "X_bank_original = np.insert(\n",
        "    X_bank_original_no_bias, 0, np.ones(X_bank_original_no_bias.shape[0]), axis=1\n",
        ")\n",
        "y_bank_original = bank_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
        "\n",
        "# Create an instance of the model object\n",
        "initial_weights = np.zeros((x_train.shape[1], 1))\n",
        "test = LogisticClassifier(initial_weights)\n",
        "\n",
        "## Shuffling function will shuffle X&Y_hep_original, copy for kfold CV to use\n",
        "X_bank_kfold = X_bank_original.copy()\n",
        "y_bank_kfold = y_bank_original.copy()\n",
        "\n",
        "cross_accuracy_list, weight_fold_list, CE_list, iteration_fold_list  = k_fold_Cross(model, X_hep_cat_kfold,\n",
        "                                                     y_hep_cat_kfold, k=5, \n",
        "                                                     index_to_standardize=index_numerical_columns,\n",
        "                                                     shuffle=True, printresult=True)\n",
        "\n",
        "# print(f\"It costs {(t_end-tStart)} seconds to finish k-fold cross validation\")\n",
        "plt.title('k-fold accuracy')\n",
        "plt.xlabel('fold number'), plt.ylabel('accuracy')\n",
        "plt.plot(cross_accuracy_list)\n",
        "plt.show()\n",
        "\n",
        "plt.title('k-fold cross entropy loss')\n",
        "plt.xlabel('fold number'), plt.ylabel('CE')\n",
        "plt.plot(CE_list)\n",
        "plt.show()\n",
        "\"\"\"\n",
        "plt.title('k-fold iteration')\n",
        "plt.xlabel('fold number'), plt.ylabel('iteration')\n",
        "plt.plot(iteration_fold_list)\n",
        "plt.show()\n",
        "\"\"\"\n",
        "print (\"iteration time\", iteration_fold_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifR1BPHQXVAf"
      },
      "source": [
        "# Loop K-fold\n",
        "X_bank_kfold = X_bank_original.copy()\n",
        "y_bank_kfold = y_bank_original.copy()\n",
        "initial_weights = np.zeros((x_train.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "k = 10\n",
        "loop = 10\n",
        "iteration_fold = []\n",
        "accuracy_fold = []\n",
        "cross_accuracy = 0\n",
        "for i in range(loop):\n",
        "  cross_accuracy_list, weight_fold_list, CE_list, iteration_fold_list= k_fold_Cross(model, X_bank_kfold,\n",
        "                                                     y_bank_kfold, k=10, \n",
        "                                                     shuffle=True, printresult=False)\n",
        "  cross_accuracy += sum(cross_accuracy_list)/k\n",
        "  accuracy_fold.append(np.around((sum(cross_accuracy_list)/k)*100,2))\n",
        "  iteration_fold.append(sum(iteration_fold_list)/k)\n",
        "  \n",
        "\n",
        "cross_accuracy = cross_accuracy/loop\n",
        "\n",
        "print(f'Average cross accuracy is {100*cross_accuracy:.2f} %')\n",
        "print (accuracy_fold)\n",
        "print (iteration_fold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn0xVMl4X12H"
      },
      "source": [
        "# Compare effect of standardization (Template?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTIfm-ydEH02"
      },
      "source": [
        "Section hepatitis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ92UzurX-kw"
      },
      "source": [
        "print(\"#####Section HEPATITIS\")\n",
        "\n",
        "# Indices of features\n",
        "index_cat_columns = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18])\n",
        "index_numerical_columns = np.array([0, 13, 14, 15, 16, 17])\n",
        "\n",
        "# Create original set of features\n",
        "X_hep_original_no_bias = hep_data.iloc[:, :-1].to_numpy()\n",
        "X_hep_original = np.insert(\n",
        "    X_hep_original_no_bias, 0, np.ones(X_hep_original_no_bias.shape[0]), axis=1\n",
        ")\n",
        "y_hep_original = hep_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
        "\n",
        "# Account for extra column for bias\n",
        "index_numerical_columns = index_numerical_columns + 1\n",
        "index_cat_columns = index_cat_columns + 1\n",
        "\n",
        "#Test group 1\n",
        "X_hep_cat_kfold = X_hep_original.copy()\n",
        "y_hep_cat_kfold = y_hep_original.copy()\n",
        "\n",
        "#Test group 2\n",
        "X_hep_cat_kfold_standardized = X_hep_original.copy()\n",
        "y_hep_cat_kfold_standardized = y_hep_original.copy()\n",
        "\n",
        "\n",
        "# Loop K-fold group 1: NOT standardized\n",
        "initial_weights = np.zeros((X_hep_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "tstart1 = time.time()\n",
        "k = 5\n",
        "loop = 10\n",
        "iteration_fold = []\n",
        "accuracy_fold = []\n",
        "cross_accuracy = 0\n",
        "for i in range(loop):\n",
        "  cross_accuracy_list, weight_fold_list, CE_list, iteration_fold_list  = k_fold_Cross(model, X_hep_cat_kfold,\n",
        "                                                     y_hep_cat_kfold, k=5, \n",
        "                                                     index_to_standardize=index_numerical_columns,\n",
        "                                                     standardize=False,\n",
        "                                                     shuffle=True, printresult=False)\n",
        "  cross_accuracy += sum(cross_accuracy_list)/k\n",
        "  accuracy_fold.append(np.around((sum(cross_accuracy_list)/k)*100,2))\n",
        "  iteration_fold.append(sum(iteration_fold_list)/k)\n",
        "\n",
        "cross_accuracy = cross_accuracy/loop\n",
        "tend1 = time.time()\n",
        "print(f'Average cross accuracy is {100*cross_accuracy:.2f} %')\n",
        "print (accuracy_fold)\n",
        "print (iteration_fold)\n",
        "print(f\"It costs {(tend1-tstart1)} seconds\")\n",
        "\n",
        "\n",
        "# Loop K-fold group 2: standardized\n",
        "initial_weights = np.zeros((X_hep_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "tstart2=time.time()\n",
        "k = 5\n",
        "loop = 10\n",
        "cross_accuracy = 0\n",
        "iteration_fold = []\n",
        "accuracy_fold = []\n",
        "for i in range(loop):\n",
        "  cross_accuracy_list, weight_fold_list, CE_list, iteration_fold_list = k_fold_Cross(model, X_hep_cat_kfold_standardized,\n",
        "                                                     y_hep_cat_kfold_standardized, k=5,\n",
        "                                                     index_to_standardize=index_numerical_columns,\n",
        "                                                     standardize=True,\n",
        "                                                     shuffle=True, printresult=False)\n",
        "  cross_accuracy += sum(cross_accuracy_list)/k\n",
        "  accuracy_fold.append(np.around((sum(cross_accuracy_list)/k)*100,2))\n",
        "  iteration_fold.append(sum(iteration_fold_list)/k)\n",
        "\n",
        "cross_accuracy = cross_accuracy/loop\n",
        "tend2=time.time()\n",
        "print(f'Average cross accuracy standardized is {100*cross_accuracy:.2f} %')\n",
        "print (accuracy_fold)\n",
        "print (iteration_fold)\n",
        "print(f\"It costs {(tend2-tstart2)} seconds\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upSY_pZ9ER05"
      },
      "source": [
        "Section Bankruptcy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f1tEpkdYBJi"
      },
      "source": [
        "print(\"#####Section Bankruptcy\")\n",
        "\n",
        "# Implement k-fold cross validation\n",
        "X_bank_original_no_bias = bank_data.iloc[:, :-1].to_numpy()\n",
        "X_bank_original = np.insert(\n",
        "    X_bank_original_no_bias, 0, np.ones(X_bank_original_no_bias.shape[0]), axis=1\n",
        ")\n",
        "y_bank_original = bank_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
        "\n",
        "# Test Group 1\n",
        "X_bank_kfold = X_bank_original.copy()\n",
        "y_bank_kfold = y_bank_original.copy()\n",
        "# Test Group 2\n",
        "X_bank_kfold_standardized = X_bank_original.copy()\n",
        "y_bank_kfold_standardized = y_bank_original.copy()\n",
        "\n",
        "\n",
        "# Loop K-fold group1: NOT standardized\n",
        "initial_weights = np.zeros((x_train.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "tstart1 = time.time()\n",
        "k = 10\n",
        "loop = 10\n",
        "iteration_fold = []\n",
        "accuracy_fold = []\n",
        "\n",
        "cross_accuracy = 0\n",
        "for i in range(loop):\n",
        "  cross_accuracy_list, weight_fold_list, CE_list, iteration_fold_list= k_fold_Cross(model, X_bank_kfold,\n",
        "                                                     y_bank_kfold, k=10, \n",
        "                                                     standardize=False,\n",
        "                                                     shuffle=True, printresult=False)\n",
        "  cross_accuracy += sum(cross_accuracy_list)/k\n",
        "  accuracy_fold.append(np.around((sum(cross_accuracy_list)/k)*100,2))\n",
        "  iteration_fold.append(sum(iteration_fold_list)/k)\n",
        "\n",
        "cross_accuracy = cross_accuracy/loop\n",
        "tend1 = time.time()\n",
        "print(f'Average cross accuracy is {100*cross_accuracy:.2f} %')\n",
        "print (accuracy_fold)\n",
        "print (iteration_fold)\n",
        "print(f\"It costs {(tend1-tstart1)} seconds\")\n",
        "\n",
        "\n",
        "# Loop K-fold group2: standardized\n",
        "initial_weights = np.zeros((x_train.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "tstart2 = time.time()\n",
        "k = 10\n",
        "loop = 10\n",
        "cross_accuracy = 0\n",
        "iteration_fold = []\n",
        "accuracy_fold = []\n",
        "for i in range(loop):\n",
        "  cross_accuracy_list, weight_fold_list, CE_list, iteration_fold_list = k_fold_Cross(model, X_bank_kfold_standardized,\n",
        "                                                     y_bank_kfold_standardized, k=10,\n",
        "                                                     standardize=False,                                                     \n",
        "                                                     shuffle=True, printresult=False)\n",
        "  cross_accuracy += sum(cross_accuracy_list)/k\n",
        "  accuracy_fold.append(np.around((sum(cross_accuracy_list)/k)*100,2))\n",
        "  iteration_fold.append(sum(iteration_fold_list)/k)\n",
        "\n",
        "cross_accuracy = cross_accuracy/loop\n",
        "tend2 = time.time()\n",
        "print(f'Average cross accuracy is {100*cross_accuracy:.2f}%')\n",
        "print (accuracy_fold)\n",
        "print (iteration_fold)\n",
        "print(f\"It costs {(tend2-tstart2)} seconds\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWpykgW9Rhqc"
      },
      "source": [
        "# Adding features (Ignore for the moment!!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BHWPKcutw6Z"
      },
      "source": [
        "#add x^2 to all features in bankruptcy\n",
        "\n",
        "rep = bank_data.shape[1]\n",
        "for i in range(rep-1):\n",
        "  bank_data.insert(bank_data.shape[1]-1, f'attribute{i+rep}', bank_data[f'attribute{i+1}']**2 )\n",
        "print(bank_data.head())\n",
        "\n",
        "# split data into training and test sets\n",
        "data_train, data_test = splitdata(bank_data, 0.8)\n",
        "\n",
        "# pre-process trainding set (adding column 0 and standardization)\n",
        "X_without_intercept = data_train.iloc[:,:-1].to_numpy()\n",
        "x_train = np.insert(X_without_intercept, 0, np.ones(X_without_intercept.shape[0]),\n",
        "              axis=1)\n",
        "x_train[:, 1:] = standardization(x_train[:,1:])\n",
        "y_train = data_train.iloc[:,-1].to_numpy().reshape(-1,1)\n",
        "\n",
        "# start clock\n",
        "tStart = time.time()\n",
        "\n",
        "# fit training set\n",
        "w_init = np.zeros((x_train.shape[1], 1))\n",
        "#w_init = np.random.rand(x_train.shape[1],1) * 100\n",
        "test = LogisticClassifier(w_init)\n",
        "test.fit(x_train, y_train, iter_time = 20000, epsilon= 1e-06)\n",
        "\n",
        "# predict with test set\n",
        "X_without_intercept_test = data_test.iloc[:,:-1].to_numpy()\n",
        "x_test = np.insert(X_without_intercept_test, 0, np.ones(X_without_intercept_test.shape[0]),\n",
        "              axis=1)\n",
        "x_test[:, 1:] = standardization(x_test[:,1:])\n",
        "y_test = data_test.iloc[:,-1].to_numpy().reshape(-1,1)\n",
        "y_pred = test.predict(x_test)\n",
        "#print(y_pred)\n",
        "\n",
        "# find out accuracy of prediction\n",
        "accuracy = test.accu_eval(y_test, y_pred)\n",
        "print(f\"The accuracy is {(accuracy)} \")\n",
        "\n",
        "# end clock\n",
        "tEnd = time.time()\n",
        "print(f\"It costs {(tEnd-tStart)} seconds\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhA7Cqs9H2ld"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYD9uZMvORYk"
      },
      "source": [
        "# Removing independent features from the hepatitis data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvFXzdkWOW0q"
      },
      "source": [
        "# Create data\n",
        "\n",
        "# Indices of features\n",
        "index_cat_columns = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18])\n",
        "index_numerical_columns = np.array([0, 13, 14, 15, 16, 17])\n",
        "\n",
        "# Create original set of features\n",
        "X_hep_original_no_bias = hep_data.iloc[:, :-1].to_numpy()\n",
        "X_hep_original = np.insert(\n",
        "    X_hep_original_no_bias, 0, np.ones(X_hep_original_no_bias.shape[0]), axis=1\n",
        ")\n",
        "y_hep_original = hep_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
        "\n",
        "# Shuffle data\n",
        "full_array = np.concatenate((X_hep_original, y_hep_original), axis=1)\n",
        "np.random.shuffle(full_array)\n",
        "X_hep_original = full_array[:,:-1]\n",
        "y_hep_original = full_array[:,[-1]]\n",
        "\n",
        "# Account for extra column for bias\n",
        "index_numerical_columns = index_numerical_columns + 1\n",
        "index_cat_columns = index_cat_columns + 1\n",
        "\n",
        "# indices of features to remove\n",
        "features_to_remove = [2, 3, 4, 8, 9, 16]\n",
        "\n",
        "initial_weights = np.zeros((X_hep_original.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "cross_accuracy_list, weight_fold_list, CE_list = k_fold_Cross(model, \n",
        "                                                              X_hep_original, \n",
        "                                                              y_hep_original, \n",
        "                                                              k=5, \n",
        "                                                              standardize=False, \n",
        "                                                              shuffle=False, \n",
        "                                                              printresult=False)\n",
        "\n",
        "print(np.mean(cross_accuracy_list))\n",
        "  \n",
        "\n",
        "for i in features_to_remove:\n",
        "  print(f\"Removing {hep_data.columns[i-1]}...\")\n",
        "  X_hep_remove = np.delete(X_hep_original, i, axis = 1)\n",
        "\n",
        "  initial_weights = np.zeros((X_hep_remove.shape[1], 1))\n",
        "  model = LogisticClassifier(initial_weights)\n",
        "\n",
        "  cross_accuracy_list, weight_fold_list, CE_list = k_fold_Cross(model, \n",
        "                                                                X_hep_remove, \n",
        "                                                                y_hep_original, \n",
        "                                                                k=5, \n",
        "                                                                standardize=False, \n",
        "                                                                shuffle=False, \n",
        "                                                                printresult=False)\n",
        "  \n",
        "  print(np.mean(cross_accuracy_list))\n",
        "\n",
        "# Remove all features\n",
        "print(f\"Removing all independent features...\")\n",
        "X_hep_remove = np.delete(X_hep_original, features_to_remove, axis=1)\n",
        "\n",
        "initial_weights = np.zeros((X_hep_remove.shape[1], 1))\n",
        "model = LogisticClassifier(initial_weights)\n",
        "\n",
        "cross_accuracy_list, weight_fold_list, CE_list = k_fold_Cross(model, \n",
        "                                                              X_hep_remove, \n",
        "                                                              y_hep_original, \n",
        "                                                              k=5, \n",
        "                                                              standardize=False, \n",
        "                                                              shuffle=False, \n",
        "                                                              printresult=False)\n",
        "\n",
        "print(np.mean(cross_accuracy_list))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9M5WJhOuxs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}