{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Miniproject1_james.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "at-eQqMKyZC2"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HungYangChang/ECSE551/blob/master/Miniproject1_james.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eird4T8uylxh"
      },
      "source": [
        "# Fetch Hepatitis Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwOxu8XFLcS7"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLdbMJpptgiB"
      },
      "source": [
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/jonarsenault/ecse551data/master/hepatitis.csv\"\n",
        "# url = \"https://raw.githubusercontent.com/jonarsenault/ecse551data/master/bankrupcy.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# # check if data was read successfully or not\n",
        "print(data.head())\n",
        "\n",
        "# data shape\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at-eQqMKyZC2"
      },
      "source": [
        "# Plot\n",
        "Task: what are the distribution of some of the features? etc. You may visualize your results using histogram plots.\n",
        "\n",
        "Plot the distribution of data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMFfeL9_xTz1"
      },
      "source": [
        "plt.subplot(1,2,1)\n",
        "sns.distplot(data['age'])\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(data['sex'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6tIe12RxWqG"
      },
      "source": [
        "plt.subplot(1,2,1)\n",
        "sns.distplot(data['steroid'])\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(data['antivirals'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy8c3jgWxZTH"
      },
      "source": [
        "plt.subplot(1,2,1)\n",
        "sns.distplot(data['fatigue'])\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(data['malaise'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARiOnY4Rx_QE"
      },
      "source": [
        "plt.subplot(1,2,1)\n",
        "sns.distplot(data['anorexia'])\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(data['liver_big'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpBS6cB8zbEw"
      },
      "source": [
        "plt.subplot(1,2,1)\n",
        "sns.distplot(data['liver_firm'])\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(data['spleen_palable'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3a5Gtn60ClL"
      },
      "source": [
        "plt.subplot(1,2,1)\n",
        "sns.distplot(data['spiders'])\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(data['ascites'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8MnUgY60Kpu"
      },
      "source": [
        "plt.subplot(1,2,1)\n",
        "sns.distplot(data['varices'])\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(data['bilirubin'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElgVch3k0cbP"
      },
      "source": [
        "plt.subplot(1,2,1)\n",
        "sns.distplot(data['alk_phosphate'])\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(data['sgot'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WghReb7y0hq9"
      },
      "source": [
        "plt.subplot(1,3,1)\n",
        "sns.distplot(data['albumin'])\n",
        "plt.subplot(1,3,2)\n",
        "sns.distplot(data['protime'])\n",
        "plt.subplot(1,3,3)\n",
        "sns.distplot(data['histology'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I71iF-Ejy1fx"
      },
      "source": [
        "# Analysize Data\n",
        "Task: Perform some statistical analysis on the datasets e.g. what are the distribution of the two classes? \n",
        "\n",
        "Find mean, std, correlation (what others?...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3MZqwFdqSqq"
      },
      "source": [
        "# Analysize Data\n",
        "# Find mean and std\n",
        "data.iloc[:,:-1].describe()\n",
        "n_colum = int(data.size/len(data))\n",
        "\n",
        "# Find correlation of each one (to check whether i.i.d assumpation is correct enough or not)\n",
        "corr_data = data.corr()\n",
        "plt.figure(figsize=(n_colum, n_colum))\n",
        "heatmap = sns.heatmap(corr_data, annot=True, linewidths=0.2, linecolor=\"white\",vmin=0.3, cmap=\"PuRd\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrWyuYTQzIfd"
      },
      "source": [
        "# Split Data: Training and Test Data\n",
        "split oriningal data to training data and Test data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkYHMLlRMDNz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ1mU5my2EMU"
      },
      "source": [
        "# split data into Training, validation, and Test (Q: is validation necessary?)\n",
        "### k-fold non-finished yet ###\n",
        "\n",
        "# 80% for training\n",
        "\n",
        "def splitdata(data,perc_training):\n",
        "    data_train = data.sample(frac = perc_training, random_state=1)\n",
        "    ##.iloc to get row information but not include label(last one)\n",
        "    x_train = np.array(data_train.iloc[:, :-1])\n",
        "    print (x_train.shape)\n",
        "    y_train = np.array(data_train[\"ClassLabel\"])\n",
        "    print (y_train.shape)\n",
        "\n",
        "    ## Select rest of data for testing\n",
        "    data_test = data.drop(data_train.index)\n",
        "    x_test = np.array(data_test.iloc[:, :-1])\n",
        "    y_test = np.array(data_test[\"ClassLabel\"])\n",
        "    print(\"Number of Training data: \", len(x_train),\"Testing data:\",len(x_test))\n",
        "    return x_train, y_train, x_test, y_test\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUqkPM4tzSBa"
      },
      "source": [
        "# linear classifier logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz7ew7S42bfD"
      },
      "source": [
        "Implementing ***Linear classifier logistic regression***\n",
        "The main aim of the Hepatitis dataset is to discriminate two classes: survivors and patients for whom the hepatitis proved terminal.\n",
        "\n",
        "You are free to implement the method in anyway you want, however we recommend to implement both models as python classes (use of constructor is recommended). Each of your model class should have at least two functions: *fit* and *predict*. The function *fit* takes the training data X and its corresponding labels vector y as well as other hyperparameters (such as learning rate) as input, and execute the model training through modifying the model parameters (i.e. W). *predict* takes a set of test data as input and outputs predicted labels for the input points. Note that you need to convert probabilities to binary 0-1 predictions by thresholding the output at 0.5. The ground-truth labels should also be converted to binary 0-1.\n",
        "\n",
        "you might investigate different stopping criteria for the gradient descent in logistic regression or develop an automated approach to select a good subset of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbFsBAQ_MaVi"
      },
      "source": [
        "linear classifier logistic regression\n",
        "1. normalization\n",
        "2. sigmoid function\n",
        "3. fit\n",
        "4. predict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BHWPKcutw6Z"
      },
      "source": [
        "## linear classifier logistic regression\n",
        "\n",
        "class Linear_logistic_classifier:\n",
        "\n",
        "  ## Initializing weights by random\n",
        "  # feed initial weight into function\n",
        "  def __init__(self, weights):\n",
        "      self.w = weights\n",
        "      pass\n",
        "\n",
        "  ## determine whether normalization (for training dta, norm = True)\n",
        "  def normalization(self, x, norm = True):\n",
        "      if norm:\n",
        "        self.x_mean  = np.mean(x)\n",
        "        self.x_std = np.std(x)\n",
        "        return (x-self.x_mean)/(self.x_std)\n",
        "      else:\n",
        "        return x\n",
        "  \n",
        "  ## sigmoid function\n",
        "  def sigmoid(self,x):\n",
        "      return 1/(1+np.exp(-x))\n",
        "\n",
        "  '''\n",
        "  ## fit function\n",
        "  lr: learning rate\n",
        "  iter_time: total times for iteration (even if it's not converge)\n",
        "  stop_criteria : If the loss is less than this value, then stop\n",
        "  '''\n",
        "  def fit(self, x_train, y_train, lr, iter_time, stop_criteria = 1e-2, norm = True, epsilon =1e-6): \n",
        "      x_train = self.normalization(x_train, norm)\n",
        "      y_train = self.normalization(y_train, norm)\n",
        "      CEstore = []\n",
        "      DCEstore = []\n",
        "      for iteration in range(iter_time):\n",
        "          wtx = np.dot(self.w.T, x_train.T)\n",
        "          # Cross-entropy(CE)\n",
        "          CE = -1 * np.sum(\n",
        "              np.sum(np.dot(y_train, np.log(self.sigmoid(wtx).T + epsilon),\n",
        "              np.dot((1-y_train), np.log((1- self.sigmoid(wtx)).T + epsilon))))\n",
        "              ).reshape(-1, 1)\n",
        "          # Derivative of C\n",
        "          DCE = -1* np.sum(\n",
        "              np.dot(x_train.T,(y_train-self.sigmoid((wtx))).T)\n",
        "              ).reshape(-1, 1)\n",
        "          # weight update\n",
        "          self.w -= lr*DCE\n",
        "          CEstore.append(CE.flatten())\n",
        "          DCEstore.append(DCE.flatten())\n",
        "\n",
        "      ### TODO stop criteria\n",
        "      return CEstore, DCEstore\n",
        "\n",
        "  ## predict function : normalize data first, and then feed x_test \n",
        "  def predict(self, x_test, y_test, norm = True):\n",
        "      x_test = self.normalization(x_test, norm)\n",
        "      y_test = self.normalization(y_test, norm)\n",
        "      \n",
        "      y_predict_prob = self.sigmoid(np.dot(self.w.T,x_test.T))\n",
        "      y_predict = np.where(y_predict_prob < 0.5,0,1)\n",
        "      return y_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbKQ777iuGk6"
      },
      "source": [
        "          CE = -1 * np.sum(\n",
        "              np.sum(np.dot(y_train, np.log(self.sigmoid(wtx).T + epsilon),\n",
        "              np.dot((1-y_train), np.log((1- self.sigmoid(wtx)).T + epsilon))))\n",
        "              ).reshape(-1, 1)\n",
        "          # Derivative of C\n",
        "          DCE = -1* np.sum(\n",
        "              np.dot(x_train.T,(y_train-self.sigmoid((wtx))).T)\n",
        "              ).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPQSMLz2sPiG"
      },
      "source": [
        "1. evlu_acc\n",
        "2. initial weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPJfq2ifsLpo"
      },
      "source": [
        "def evalu_acc(y_test, y_predict):\n",
        "    return np.mean(y_test == y_predict)\n",
        "\n",
        "def initial_weights(x_train, init_constant=0.3):\n",
        "    weights = np.array(init_constant*np.random.rand(x_train.shape[1],1))\n",
        "    return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzw8P5iAMoz1"
      },
      "source": [
        "Run the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KJwJB4Q5rXp"
      },
      "source": [
        "# split data\n",
        "x_train, y_train, x_test, y_test = splitdata(data,0.8)\n",
        "\n",
        "# generate weights\n",
        "weights = initial_weights(x_train, 0.2)\n",
        "print (\"type:\", weights.shape)\n",
        "\n",
        "# Use Classifier\n",
        "reg = Linear_logistic_classifier(weights)\n",
        "CE, DCE = reg.fit(x_train, y_train, 0.02, 500, 0.01, True, 1e-6)\n",
        "y_predict = reg.predict(x_test, y_test, True)\n",
        "accuracy = evalu_acc(y_test, y_predict)\n",
        "print (\"the accuracy is\", accuracy)\n",
        "print (CE)\n",
        "\n",
        "plt.title('CE function')\n",
        "plt.xlabel('iterations'), plt.ylabel('CE function')\n",
        "plt.plot(CE)\n",
        "plt.show()\n",
        "\n",
        "print (DCE)\n",
        "\n",
        "plt.title('CE function')\n",
        "plt.xlabel('iterations'), plt.ylabel('CE function')\n",
        "plt.plot(DCE)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41h96mdbH8rU"
      },
      "source": [
        "# k-fold\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmafTQN93i_P"
      },
      "source": [
        "# Changing learning rate, check if it's overfit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUTqwUglzfaY"
      },
      "source": [
        "# Report.pdf\n",
        "•   Does the report include all the required experiments?\n",
        "•  Is the report technically sound? (i.e. do the steps taken make sense? Are the results\n",
        "in an acceptable range?)\n",
        "•  How thorough/rigorous is the experimental validation?\n",
        "•  Is the report well-organized and coherent?\n",
        "•  Is the report clear and free of grammatical errors and typos?\n",
        "•  Does the report contain sufficient and appropriate references and related work?\n"
      ]
    }
  ]
}